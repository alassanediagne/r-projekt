---
title: "Clustering Paket"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Clustering Paket}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Clustering)
library(ggplot2)
library(dplyr)
set.seed(13)
```

This package implements the main clustering algorithms.

Clustering is a key technique in unsupervised learning, because it helps in discovering structure in data.

<<<<<<< HEAD
Unsupervised learning aims to find patterns in a training set $X \in \mathbb{R}^{n\times d}$ with instances $X_i \in \mathbb{R}^d, i=1,...,n$ without any (or minimal) external information and in some cases can even make predictions based on that.
=======
In supervised learning, the best known sub discipline of machine learning, the aim is to predict the output $\bar{Y}$ of a model given the input variables $\bar{X}$. We achieve this by training a machine learning algorithm on training data $(X_i, Y_i), i=1,..,n$ to which we know the response.

In contrast, unsupervised learning aims to find patterns in a training set $X_i, i=1,...,n$ without any (or minimal) external information and in some cases can even make predictions based on that.
>>>>>>> ab198a523b077f911597299978445b901477e856

This is best illustrated by an example. Let us take a look the `iris` data set. It contains the sepal lengths and widths, petal lengths and widths and species of different irises. If you take a closer look at the petal lengths and widths in relation to the species, we find that they are closely correlated:

```{r, fig.show='hold'}
ggplot() + 
  geom_point(data=iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) +
  theme_bw()
```

Based on this plot we can conduct that irises of the species virginica tend to have large petals while irises of the species setosa have rather smaller petals. In the above plot these species form their own clusters.

The objective of clustering would be to find these clusters - without knowing about the species of the instances. It does not help us telling, which species a specific iris belongs to. But it would tell us that these irises have something in common and some algorithms even allow us to predict, given a petal length and width, which cluster a new instance would belong to.

There are multiple clustering algorithms, each with its different strengths and weaknesses which will be illustrated here.

## k-means clustering

`k_means` clustering is an iterative algorithm and the most widespread clustering algorithm. Having complexity $O(n)$ it is very efficient even for huge data sets.

The algorithm takes besides the data an integer `k` (in this package for better readability `num_cluster`) as input, which sets the desired number of clusters. The algorithm then finds `k` approximately optimal cluster centers and each point in the data set is being assigned to the closest cluster centroid. This optimality is with respect to the minimal euclidian distance between clusters, resulting in that the points which are very close to each other will most likely get clustered together.

The k-means algorithm expects a matrix as input. To come back to the example in the beginning, we are going to take a look at the petal sizes in the iris data set.

As an unsupervised learning algorithm, `k_means` only needs the petal length and width as input which we transform into a matrix:

```{r}
irisPetals <- iris |> select(c(Petal.Length,Petal.Width)) |> data.matrix()
```

Now we need the second input for the `k_means` function, the desired number of clusters. Here we run into the main bottle neck of k-means clustering: we need to manually choose the best number for `num_cluster`. Therefore we do need some additional information about the data if we do not want to set the parameter by trial and error.

In this case we have seen a plot of the data and know, that clustering in the data into three clusters is sensible.

```{r}
k_means(irisPetals, 3)
```

Apart from the cluster means, the k-means algorithm also returned a logical indicating that it converged and the number of iterations it needed till convergence. Optionally we could also retrieve the cluster means in every iteration.

The package also implemented a function which performs k-means clustering on 2d data and immediately plots the clusters using ggplot:

```{r}
plot_k_means_2d(irisPetals, 3)
```

As we can see, this plot resembles the plot we saw above a lot. Apart of a few outliers, the algorithm has clustered the right instances together.

We can now also predict, to which cluster two irises with petal lengths $2$ and $6$ and petal widths $0.5$ and $2.7$ would belong to:

```{r}
petals_km <- k_means(irisPetals, 3)
new_irises <- rbind(c(2,0.5), c(6,2.7))
k_means_predict(new_irises, petals_km$means)
```

Of course, to make sense of these labels, we have to take a look at the means and see which other instances have been clustered to this group:

```{r}
petals_km
```

It is important to note that the algorithm uses a probabilistic method (called `k-means ++`) to find good initial values and assure fast convergence, thus the results may vary if the code is executed multiple times (i.e. slightly different results, rows in \$means swapped) . This can be prevented by `set.seed()`.
