---
title: "Clustering Paket"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Clustering Paket}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Clustering)
library(ggplot2)
library(dplyr)
set.seed(13)
```

This package implements the main clustering algorithms.

Clustering is a key technique in unsupervised learning.

Unsupervised learning aims to find patterns in a training set $X \in \mathbb{R}^{n\times d}$ with instances $X_i \in \mathbb{R}^d, i=1,...,n$ without any (or minimal) external information and in some cases can even make predictions based on that.

This is best illustrated by an example. Let us take a look the `iris` data set. It contains the sepal lengths and widths, petal lengths and widths and species of different irises. If you take a closer look at the petal lengths and widths in relation to the species, we find that they are closely correlated:

```{r, fig.show='hold'}
ggplot() + 
  geom_point(data=iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) +
  theme_bw()
```

Based on this plot one can easily conduct that irises of the species virginica tend to have large petals while irises of the species setosa have rather smaller petals. In the above plot these species form their own clusters.

The objective of a clustering algorithm would be to find these clusters without any previous knowledge of the data. And some algorithms even allow us to predict, given a petal length and width, which cluster a new instance would belong to.

There are multiple clustering algorithms, each with its different strengths and weaknesses which will be illustrated here.

## k-means clustering

`k_means` clustering is an iterative algorithm and the most widespread clustering algorithm. Having complexity $O(n)$ it is very efficient even for huge data sets.

The algorithm takes, besides the data, an integer `k` (in this package for better readability `num_cluster`) as input, which sets the desired number of clusters. The algorithm then finds `k` approximately optimal cluster centers and each point in the data set is being assigned to the closest cluster centroid. This optimality is with respect to the minimal euclidian distance between clusters, resulting in that the points which are very close to each other will most likely get clustered together.

The k-means algorithm expects a matrix as input. To come back to the example in the beginning, we are going to take a look at the petal sizes in the iris data set.

As an unsupervised learning algorithm, `k_means` only needs the petal length and width as input which we transform into a matrix:

```{r}
irisPetals <- iris |> select(c(Petal.Length,Petal.Width)) |> data.matrix()
```

Now we need the second input for the `k_means` function, the desired number of clusters. Here we run into the main bottle neck of k-means clustering: we need to manually choose the best number for `num_cluster`. Therefore we do need some additional information about the data if we do not want to set the parameter by trial and error.

In this case we have seen a plot of the data and know, that clustering in the data into three clusters is sensible.

```{r}
k_means(irisPetals, 3)
```

Apart from the cluster means, the k-means algorithm also returned a logical indicating that it converged and the number of iterations it needed till convergence. Optionally we could also retrieve the cluster means in every iteration.

The package also implemented a function which performs k-means clustering on 2d data and immediately plots the clusters using ggplot:

```{r}
plot_k_means_2d(irisPetals, 3)
```

As we can see, this plot resembles the plot we saw above a lot. Apart of a few outliers, the algorithm has clustered the right instances together.

We can now also predict, to which cluster two irises with petal lengths $2$ and $6$ and petal widths $0.5$ and $2.7$ would belong to:

```{r}
petals_km <- k_means(irisPetals, 3)
new_irises <- rbind(c(2,0.5), c(6,2.7))
k_means_predict(new_irises, petals_km$means)
```

Of course, to make sense of these labels, we have to take a look at the means and see which other instances have been clustered to this group:

```{r}
petals_km
```

It is important to note that the algorithm uses a probabilistic method (called `k-means ++`) to find good initial values and assure fast convergence, thus the results may vary if the code is executed multiple times (i.e. slightly different results, rows in `$means` swapped) . This can be prevented by `set.seed()`.

## Overview OPTICS algorithm

The `Clustering` package includes an implementation of the Ordering points to identify the clustering structure (OPTICS) algorithm.

It is an algorithm for finding density based clusters in spatial data. 

It's basic idea is similiar to the DBSCAN algorithm, both algorithm 'group' objects, by identifying core points with sufficient neighbors and expand clusters from these core points.

However, while DBSCAN uses a fixed radius to form clusters, OPTICS builds an ordering of the data points and a reachability to handle varying densities of clusters more flexibly.

The clusters then can be extracted later on.

A broader overview to the functionality of this algorithm can be found [here](https://de.wikipedia.org/wiki/OPTICS).

### Implementation

In this package the OPTICS algorithm can be called as a function 'optics'.

As input it takes `data` a matrix in $\mathbb{R}^n \times \mathbb{R}^d$, where each row represents a data point.
A second input is a parameter `eps` in $\mathbb{R}^+$, which is used to find the neighborhood of a point using the euclidean distance.
A third input is `minPts`, the minimum number of data points q in the neighborhood of a data point p (including p itslef), such that p is a core point.

The return of this function is a list of 3 elements, an `ordered_list` that returns an ordering of the indices of the `data` points as obtained by the algorithm. A vector `reachability` which stores the reachability distances of the data points in the original order of the data and the eps value that was used to run `optics`.


In the following we present some examples of the implemented algorithm and further
related functions.

```{r}
irisPetals <- iris |> dplyr::select(c(Petal.Length,Petal.Width)) |> data.matrix()
```

The optics algorithm can be run to obtain the exact clustering as DBSCAN. In this case we use the same `eps`,
the same `minPts`. As the following example using the `iris` data set shows.

```{r, fig.width=6, fig.height=4, echo=T,message=F, eval = FALSE}
eps <- 0.5
minPts <- 3

optics_result <- optics(irisPetals, 0.5, 3)

plot_optics_2d(irisPetals, optics_result)

```
<div style="text-align: center;">
  <img src="figures/optics_iris.png" style="width:60%;" alt="irisPetals clustered with OPTICS">
</div>

When using `optics` in a more typical fashion, a rather large `eps` value (e.g. 10) is used to obtain smoother reachability distances. For this we regard some more data

```{r, fig.width=6, fig.height=5, echo=T, message=F, eval= F}
set.seed(2)
n <- 400
x <- cbind(x = runif(4, 0, 1) + rnorm(n, sd = 0.1),y = runif(4, 0, 1) + rnorm(n, sd = 0.1) )

res <- optics(x, 10, 6)

plot_reachability(res)
```
<div style="text-align: center;">
  <img src="figures/reach_plot1.png" style="width:60%;" alt="Reachability with eps = 10,, minPts = 6">
</div>

There are different approaches to extracting the density clusters from the reachability distances obtained.
In this package there is only a relatively 'simple' function `extract_dbscan` implemented, which identifies clusters by 'horizontally' cutting the reachability plot with an `eps_prime` value in $[0,\text{eps}]$.

This function works, but for a few border points. It is also callable when using the `plot_reachability` function.

For the example above we obtain a relatively good clustering, if we use a value of `eps_prime` = 0.05 .

```{r, fig.width=6, fig.height=5, echo=T,message=T, eval=FALSE}
plot_reachability(optics_result = res, extract_dbscan = TRUE, eps_prime = 0.05)
```
<div style="text-align: center;">
  <img src="figures/reach_plot2.png" style="width:60%;" alt="Reachability with eps_prime = 0.05">
</div>

For further visualization, one can plot these clusters for data with points in 2 dimensions with the function `plot_optics_2d`.

The function `extract_dbscan` is also executed when calling this function. So one can also input an `eps_prime` value. The default is the `eps` value from the result of the
optics algorithm.


```{r, fig.width=6, fig.height=4, echo=T,message=T, eval =FALSE}
plot_optics_2d(x, optics_result = res, eps_prime = 0.05)
```
<div style="text-align: center;">
  <img src="figures/optics_1.png" style="width:60%;" alt="Cluster with eps_prime = 0.05">
</div>

That different `eps_prime` will extract different clusterings can be seen, when for example chosing `eps_prime` = 0.07 .

```{r, fig.width=6, fig.height=4, echo=T,message=T, eval =FALSE}
plot_optics_2d(x, optics_result = res, eps_prime = 0.07)
```
<div style="text-align: center;">
  <img src="figures/optics_2.png" style="width:60%;" alt="Cluster with eps_prime = 0.07">
</div>

```{r, fig.width=6, fig.height=5, echo=T,message=T, eval=FALSE}
plot_reachability(optics_result = res, extract_dbscan = TRUE, eps_prime = 0.07)
```
<div style="text-align: center;">
  <img src="figures/reach_plot3.png" style="width:60%;" alt="Reachability with eps_prime = 0.07">
</div>

Extracting clusters with different `eps_prime` values allows us to identify clusters of different density, after running `optics` just once.
