k_means_pp <- function(data, num_cluster){
  n <- nrow(data)
  d <- ncol(data)

  init_vals <- matrix(0, ncol=d, nrow=num_cluster) #hier werden die Anfangswerte gespeichert

  random_idx <- sample(1:n, 1)
  init_vals[1,] <- data[random_idx, ] # erster Anfangswert ist ein zufaelliger Punkt

  eucl_dist <- function(data, point) {
    # Funktion zur Berechnung des Abstands
    return(sqrt(rowSums((data - point) ^ 2)))
  }

  min_distances <- eucl_dist(data, init_vals[1,])

  for(i in 2:num_cluster){
    prob <- min_distances^2
    prob <- prob/sum(prob)
    new_idx <- sample(1:n, 1, prob = prob)
    # Wahrscheinlichkeit ist gewichtet, proportianal zu quadriertener Distanz
    init_vals[i,] <- data[new_idx,]
    current_distances <- eucl_dist(data, init_vals[i,])
    dist_to_update <- current_distances < min_distances
    min_distances[dist_to_update] <- current_distances[dist_to_update]
  }
  return(init_vals)
}


update_C <- function(x, m){
  # x dxn Matrix mit Daten, m dxK Matrix mit aktuellen means

  n <- ncol(x)
  d <- nrow(x)
  num_cluster <- ncol(m)

  # initalisiere Variablen
  # Wir setzen die aktuellen argmins und minimalen Distanzen auf das erste mean
  current_arg_mins <- rep(1L, times=n) # hier speichern wir die aktuellen argmins
  current_min_dist <- x - matrix(m[,1], nrow = d, ncol = n)
  current_min_dist <- apply(current_min_dist, 2, \(x) sum(x^2,na.rm = TRUE)) # hier speichern wir die aktuellen minimalen Distanzen

  for(k in 2:num_cluster){
    # gehe alle means ab dem zweiten durch
    current_mean <- matrix(m[,k], nrow = d, ncol = n)
    dist_to_mean <- x - current_mean #subrahiere den aktuellen mean
    dist_to_mean <- apply(dist_to_mean, 2, \(x) sum(x^2)) # bestimme alle quadrierten eukl. Normen
    to_update <- dist_to_mean < current_min_dist # bestimme die Indizes zu denen eine kleinere Distanz gefunden wurde
    current_arg_mins[to_update] <- rep(k, times=sum(to_update)) # update argmins
    current_min_dist[to_update] <- dist_to_mean[to_update] # update minimale Distanzen
  }
  return(current_arg_mins)
}



update_m <- function(x,C,num_cluster){
  # x dxn Matrix mit Daten, C: Vektor mit aktuellen argmins, num_cluster: Anzahl der Cluster
  n <- ncol(x)
  d <- nrow(x)
  m <- matrix(nrow=d, ncol=num_cluster)

  for(k in 1:num_cluster){
    mask <- C == k # pruefe fuer welche der x_i ob k aktuelles argmin ist
    if(sum(mask)==0){
      # Falls aktuell keine Punkte dem Cluster zugewiesen sind nehme zufÃ¤lligen Clustermittelpunkt
      random_idx <- sample(1:n, 1)
      m[,k] <- x[,random_idx]
    }
    else{
      mask <- mask |> rep(each=d) |> matrix(ncol=n, nrow=d)
      x_relevant <- x[mask] |> matrix(nrow=d) # maskiere alle x_i deren aktuellen argmin nicht k ist
      m[,k] <- apply(x_relevant, 1, mean,na.rm=TRUE) # update mean
    }
  }
  return(m)
}

#'@name k_means
#'@title k-Means-Algorithmus
#'@description Implementierung des k-Means-Algorithmus aus Richter mit zusaetzlich automatischer Startwertwahl
#'@param data Matrix mit Daten die geclustered werden sollen. Jede Zeile enthaelt einen Messwert in R^d
#'@param num_cluster int. Anzahl der Cluster
#'@param m0 (optional) Matrix mit Anfangswerten zur Initialisierung des Algorithmus. Falls nicht angegeben werden die Startwerte automatisch gewaehlt
#'@param save_history (optional) logical. Gibt Cluster-Mittelpunkte und Labels in jeder Iteration zurueck. Default: FALSE
#'@param return_labels (optional) logical. Gibt zu jedem Messwert das Clusterlabel zurueck. Default: FALSE
#'@param max_iter (optinal) int. Legt maximale Anzahl an Iterationen fest. Default: 50
#'@param tol (optinal) float. Toleranz zur Festlegung der Konvergenz. Default: 1e-8
#'@param verbose (optional) logical. Falls TRUE wird die Anzahl der Iterationen und Konvergenz als Nachricht ausgegeben
#'@return Liste mit Konvergenz logical, Anzahl an Iterationen, Clustermittelpunkten, sowie, falls erwuenscht Labels, einzelnen Iterationen und Nachricht
#'@examples
#' data <- gen_clusters(50, matrix(c(0,1,2,1,0,1,2,0),ncol=2), 0.3)
#' k_means(data,4)
#'@examples
#' data <- gen_clusters(100, matrix(c(0,0,1,1,1,0,0,1), ncol=2),0.3)
#' plot_k_means_2d(data,4)
#'
#'@export

k_means <- function(data, num_cluster, m0 = NULL, save_history = FALSE,
                    return_labels=FALSE, max_iter = 50L, tol = 1e-8, verbose = FALSE){


  if(is.null(m0)){
    m0 <- k_means_pp(data, num_cluster)
  }

  data <- t(data)

  n_iter <- 0L # zaehlt Iterationen

  m <- t(m0)

  if(save_history){
    history <- list()
  }

  converged <- FALSE

  while(n_iter <= max_iter){
    m_old <- m # speichere m zum vergleichen
    current_arg_mins <- update_C(data,m) # update argmins
    m <- update_m(data,current_arg_mins,num_cluster) # update means

    if(save_history){
      history <- append(history,list(iteration = n_iter, means = t(m), argmins=current_arg_mins))
    }

    if(norm(m - m_old, type="i") < tol) {
      # pruefe konvergenz
      converged <- TRUE
      break
    }

    n_iter <- n_iter + 1L
  }

  out <- list()

  out$converged <- converged

  out$niter <- n_iter

  out$means <- t(m)

  if(save_history){
    out$history <- history
  }

  if(return_labels){
    out$labels <- current_arg_mins
  }

  if(verbose){
    if(converged){
      out$msg <- sprintf("Methode konvergiert nach %i Iterationen", n_iter)
    }
    else{
      out$msg <- sprintf("Maximale Anzahl an Iterationen erreicht")
    }
  }

  return(out)
}




#' k_means_predict
#'
#' @param x Ein atomarer Vektor oder eine Matrix mit Messwerten
#' @param means Clustermittelpunkte (z.B. aus dem k-Means-Algorithmus)
#'
#' @return erwartete Klassenlabel der neuen Messwerte
#' @export
#'
#' @examples data <- gen_clusters(50, matrix(c(0,1,2,0,1,2)), 0.3)
#'  clustering <- k_means(data,3)
#'  k_means_predict(c(1.2,0.8), clustering$means)
#'
k_means_predict <- function(x, means){
  predict_instance <- function(x, means){
    d <- length(x)
    if(d != ncol(means)){
      stop(sprintf("x hat nicht die richtige Dimension (%i vs %i)", ncol(means),d))
    }
    num_cluster <- nrow(means)
    x <- matrix(rep(x, each=num_cluster), ncol=d)
    dists <- abs(x-means)
    dists <- dists |> apply(1,\(x) sqrt(sum(x^2)))
    pred_label <- which.min(dists)
    return(pred_label)
  }
  if(is.atomic(x) && !is.matrix(x)){
    return(predict_instance(x,means))
  }
  else if(is.matrix(x)){
    return(apply(x, 1, \(xi) predict_instance(xi,means)))
  }
  else{
    stop(sprintf("x muss ein Vektor oder eine Matrix sein, ist jedoch von der Klasse %s", class(x)))
  }
}

plot_k_means_2d <- function(data, num_cluster, max_iter=50L){
  clustering <- k_means(data, num_cluster, return_labels = T, max_iter = max_iter)
  data <- tibble::tibble(x=data[,1], y= data[,2])
  means <- tibble::tibble(x=clustering$means[,1], y = clustering$means[,2])
  ggplot2::ggplot() +
    ggplot2::geom_point(data = data, ggplot2::aes(x = x, y = y, color = factor(clustering$labels)), size=1) +
    ggplot2::geom_point(data = means, ggplot2::aes(x = x, y = y), color="red", shape="x", size=5) +
    ggplot2::theme_bw() +
    ggplot2::theme(legend.position="none")
}

